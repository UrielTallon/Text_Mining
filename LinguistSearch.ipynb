{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lingbuzz Scrapper\n",
    "\n",
    "This python notebook displays some code for scrapping the [Lingbuzz](http://ling.auf.net/lingbuzz) website in order to generate a linguistic-related corpus. The code runs on python 2.7 without a hitch. A few modifications are necessary to ensure compatibility with python 3.4.\n",
    "\n",
    "Lingbuzz is a repository for linguistic papers pertaining to different fields and/or languages. Each paper has a unique ID coded on 6 digits and can be downloaded as a `.pdf`. The website is, unfortunately, surprisingly quirky and unstable and while navigating through it, one may often stumble upon the dreaded _HTTP Error 503: Service Temporarily Unavailable_. The actual scrapping operation may require to be run iteratively in order to ensure the whole corpus is downloaded.\n",
    "\n",
    "Lingbuzz boasts more than 5000 entries but I was able to retrieve only ~2700 of them. The root cause is still under investigation (in the future, maybe try `scrapy`).\n",
    "\n",
    "The generated corpus is saved as a JSON file which can be used to generate a dictionary. The structure of each entry is:\n",
    "\n",
    "* A unique ID as a `string` coded on 6 digits from 0 to 9.\n",
    "    + _cnt_: the number of times the related document was downloaded (`int`).\n",
    "    + _kwd_: a list of keywords related to the document (`[string]`).\n",
    "    + _tit_: the title of the document (`string`)\n",
    "    + _pub_: where the document has been published (`string`); if the document hasn't been published, then it is `N/A`.\n",
    "    + _dat_: the date of publication on Lingbuzz (`string`).\n",
    "    + _aut_: a list containing the names of the authors (`[string]`).\n",
    "    + _ref_: the document ID, same as above (`string`).\n",
    "    + _exc_: a short excerpt, detailing the work presented in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Preliminaries\n",
    "\n",
    "This part is used to import the necessary packages and define some helper functions. The imports have been tuned with `try/except` blocks so as to ensure the compatibility with Python 3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "    \n",
    "try:\n",
    "    import http.client as httplib\n",
    "except ImportError:\n",
    "    import httplib\n",
    "    \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a corpus already exists, then it is loaded. Otherwise, an empty dictionary is generated, ready to store the data.\n",
    "\n",
    "_(NOTA: to ensure compatibility with python 3.4, replace 'rb' with 'r')_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus successfuly loaded!\n",
      "Number of entries: 2723\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('Data/lingbuzzCorpus.json', 'rb') as infile:\n",
    "        ref_dict = json.load(infile)\n",
    "        print('Corpus successfuly loaded!')\n",
    "        print('Number of entries: {0}'.format(len(ref_dict.keys())))\n",
    "except IOError:\n",
    "    ref_dict = dict()\n",
    "    print('No existing corpus found, empty dictionary generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the user-agent may be useful as some servers react poorly when confronted to the default python user-agent. We set the user-agent as Mozilla's. The `url_base` variable holds the basic address of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:37.0) Gecko/20100101 Firefox/37.0\"\n",
    "url_base = \"http://lingbuzz.auf.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Scrapping\n",
    "\n",
    "The following 3 helper functions perform all the work:\n",
    "\n",
    "* `fetchDocIdAndNextPage` takes an url (`string`) as a parameter and returns a tuple `(id_list, next_page)` where `id_list` is a list of document IDs found on the page and `next_page` is part of the url leading to the next page. The parser used is `html5lib` because, though relatively slower than other parser, it is extremely consistent and prevents the loss of information.\n",
    "* `fetch_doc` takes a document ID (`string`) as a parameter and returns the related webpage, ready to be parsed by BeautifulSoup.\n",
    "* `parse_doc` takes the document returned by `fetch_doc` and the document ID as a `string`; the document is parsed with BeautifulSoup and the useful information is extracted and stored in a dictionary. Said dictionary is returned by the function.\n",
    "\n",
    "_NOTA: to ensure compatibility with python 3.4:_\n",
    "* _replace all instances of `except httplib.IncompleteRead, e:` with `except httplib.IncompleteRead as e:`._\n",
    "* _replace `except ValueError, v:` with `except ValueError as v:`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetchDocIdAndNextPage(url):\n",
    "    \n",
    "    req = urllib2.Request(url)\n",
    "    req.add_header(\"User_Agent\", user_agent)\n",
    "    response = urllib2.urlopen(req)\n",
    "    \n",
    "    try:\n",
    "        page = response.read()\n",
    "    except httplib.IncompleteRead, e:\n",
    "        page = e.partial\n",
    "    response.close()\n",
    "    \n",
    "    id_list = list()\n",
    "    next_page = ''\n",
    "    bs = BeautifulSoup(page, \"html5lib\")\n",
    "    \n",
    "    for item in bs.find_all('a'):\n",
    "        match_id = re.search(r'\\d{6}$', item.get(\"href\"))\n",
    "        match_lk = re.search(r'Next \\d+ articles', item.get_text(strip = True))\n",
    "        if match_id:\n",
    "            id_list.append(match_id.group())\n",
    "        elif match_lk:\n",
    "            next_page = item.get(\"href\")\n",
    "    \n",
    "    return (id_list, next_page)\n",
    "\n",
    "def fetch_doc(doc_id):\n",
    "    \n",
    "    doc_req = urllib2.Request(url_base + \"/lingbuzz/\" + doc_id)\n",
    "    doc_req.add_header(\"User_Agent\", user_agent)\n",
    "    doc_response = urllib2.urlopen(doc_req)\n",
    "    \n",
    "    try:\n",
    "        doc = doc_response.read()\n",
    "    except httplib.IncompleteRead, e:\n",
    "        doc = e.partial\n",
    "    doc_response.close()\n",
    "\n",
    "    return doc\n",
    "\n",
    "def parse_doc(html_doc, doc_id):\n",
    "    \n",
    "    parsed = BeautifulSoup(html_doc, \"html5lib\")\n",
    "    \n",
    "    if parsed:\n",
    "        article = dict()\n",
    "        article[\"ref\"] = doc_id\n",
    "        text_head = parsed.body.center.get_text(\"\\n\", strip = True).lower().split(\"\\n\")\n",
    "        text_body = parsed.body.get_text(\"\\n\", strip = True).lower().split(\"\\n\")\n",
    "        article[\"tit\"] = text_head[0]\n",
    "        article[\"dat\"] = text_head[-1]\n",
    "        article[\"aut\"] = list()\n",
    "        \n",
    "        for val in text_head[1:-1]:\n",
    "            if val != \",\":\n",
    "                article[\"aut\"].append(str(val.encode(\"ascii\", \"ignore\")))\n",
    "        \n",
    "        try:\n",
    "            article[\"pub\"] = text_body[text_body.index(\"published in:\")+1]\n",
    "        except ValueError, v:\n",
    "            article[\"pub\"] = 'N/A'\n",
    "        \n",
    "        article[\"kwd\"] = text_body[text_body.index(\"keywords:\")+1]\n",
    "        article[\"cnt\"] = text_body[text_body.index(\"downloaded:\")+1]\n",
    "        text_bits = list()\n",
    "        \n",
    "        for tb in text_body[text_body.index(article[\"dat\"])+1:text_body.index(\"format:\")]:\n",
    "            text_bits.append(tb)\n",
    "        article[\"exc\"] = ' '.join(text_bits)\n",
    "        \n",
    "        return article\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below may need to be run several times, as the _Error 503_ may pop up from time to time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=iEVU1YfGUtL7_g76&start=31&74\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=_sMLn7_8XjZuQ8qf&start=131&186\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=KP3dwKIpL1VdmSfS&start=231&169\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=cYQGFOyNiM9PcOlP&start=331&181\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=Xj0-j0uO4ea2nzmS&start=431&195\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=1WpDW1_N6vHPVI6d&start=531&173\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=j5-mDWwX74lv3mLs&start=631&191\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=J44-RLTkYesfoNcb&start=731&167\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=BmPtWBlqKylPpkfj&start=831&174\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=wV4xxDejtLIS6TWf&start=931&191\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=XaQBXxUQ5LoeBGJp&start=1031&180\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=LGaGzQ5aj476HX_s&start=1131&199\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=PEvOyNdiRdXzihZj&start=1231&224\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=aeG4vMFKczi_bN9r&start=1331&187\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=BpZ_GOVizCp7Wo6K&start=1431&240\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=aLW2hfk80GFg-gSf&start=1531&185\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=e0LR4-L2K9Wcb7U7&start=1631&200\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=TDBFvC5iASSYqYFN&start=1731&189\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=Nm4rPFIrsIjhuAWQ&start=1831&190\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=CDZf8Tbr6S71q4ow&start=1931&219\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=_sT3P_C6pMa4Tq5b&start=2031&215\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=KNBLomfyaL1in3df&start=2131&230\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=PzFCdPNSgW-tnqpr&start=2231&182\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=F6IT_4OYP2L1qnRB&start=2331&194\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=NX-keuI4MzZOyhH3&start=2431&243\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=1qpj-qzpFRiElztT&start=2531&221\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=8aANznY_gFuke7yn&start=2631&211\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=r7ytQlrqa7vtk8ic&start=2731&222\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=h4DhH8mufBJ_9Pj9&start=2831&250\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=FyJPCkzj3B5Is5sg&start=2931&212\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=suyIBkRarJd5ulWq&start=3031&227\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=fjtg7UfzAi2Sh4SX&start=3131&210\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=Zg_kfPSxaaxdQLQF&start=3231&246\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=3HB5aZaV22HSsfqN&start=3331&234\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=sQjAtREKXuxMWBwY&start=3431&251\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=kRGbXbA_rl3qKd-V&start=3531&240\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=oL5Kf1o1xZSRGtxT&start=3631&248\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=LLkV_GZ3HeHryfsi&start=3731&263\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=sxwHjmlfCUvw0509&start=3831&266\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=mA08RVGIpFZsWY_v&start=3931&257\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=f4TZyo7trajKRfJr&start=4031&269\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=sCskO1aM5pIcCCH-&start=4131&271\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=d68WIy8ZwKld7Cq4&start=4231&249\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=HxS-5Db9yOUbGmPG&start=4331&257\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=_F75uGJL2-oBmvY1&start=4431&277\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=BI-ZqbtneeeNBVJR&start=4531&298\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=CIuBpnTurlZwN1ZH&start=4631&290\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=MgDLSVQ_CzZ9pgjo&start=4731&293\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=kiKBKrMpA19R9GPK&start=4831&308\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=KHV1U5gdcPx6XDg1&start=4931&300\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=Y-eQ9B2blaRCzzgs&start=5031&290\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=cwf-i71T8nkfonPV&start=5131&302\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=IlHai1Fkgxx9Ng9b&start=5231&306\n",
      "Processing...\thttp://lingbuzz.auf.net/lingbuzz?_s=yOMERdpG_3N09Dtf&_k=bT4_lwmC2EaY2LXn&start=5331&309\n",
      "done; compiled 2723 entries\n"
     ]
    }
   ],
   "source": [
    "next_url = '/lingbuzz'\n",
    "next_page = url_base + next_url\n",
    "list_url = list()\n",
    "while True:\n",
    "    print('Processing...\\t{0}'.format(next_page))\n",
    "    id_list, next_url = fetchDocIdAndNextPage(next_page)\n",
    "    if next_url == '':\n",
    "        break\n",
    "    else:\n",
    "        next_page = url_base + next_url\n",
    "        list_url.append(next_url)\n",
    "    for id in id_list:\n",
    "        if id in ref_dict.keys():\n",
    "            continue\n",
    "        else:\n",
    "            ref_dict[id] = parse_doc(fetch_doc(id), id)\n",
    "print('done; compiled {0} entries'.format(len(ref_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary processing:\n",
    "\n",
    "* Change the number of downloads into an integer.\n",
    "* Remove punctuation markers in excerpt and change to lowcase. \n",
    "* Change the keywords entry into a list of lowcase keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in ref_dict.keys():\n",
    "    \n",
    "    if type(ref_dict[k]['cnt']) != int: \n",
    "        ref_dict[k]['cnt'] = int(re.search(r'\\d+', ref_dict[k]['cnt']).group())\n",
    "    \n",
    "    ref_dict[k]['exc'] = re.sub(r'[;:.\\,()]', '', ref_dict[k]['exc']).lower()\n",
    "\n",
    "    if type(ref_dict[k]['kwd']) == list:\n",
    "        ref_dict[k]['kwd'] = [kw.lower() for kw in ref_dict[k]['kwd']]\n",
    "    else:\n",
    "        keywords = list()\n",
    "        for kw in ref_dict[k]['kwd'].split(','):\n",
    "            keywords.append(kw.strip().lower())\n",
    "        ref_dict[k]['kwd'] = keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Save Corpus\n",
    "\n",
    "Save the corpus a JSON file.\n",
    "\n",
    "_(NOTA: to ensure compatibility with python 3.4, replace 'wb' with 'w')_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Data/lingbuzzCorpus.json', 'wb') as output:\n",
    "    json.dump(ref_dict, output, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
